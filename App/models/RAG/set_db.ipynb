{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set DB\n",
    "\n",
    "- Chroma DB 를 통해 VectorStore 를 구축한다.\n",
    "- 벡터 저장소 생성 from_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from load_embedding import load_embedding\n",
    "from pdf_preprocessing import LegalText\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def load_pdf_documents(pdf_dir):\n",
    "    \"\"\"PDF 파일들을 읽어서 Document 객체 리스트로 변환합니다.\"\"\"\n",
    "    filenames = glob(f'{pdf_dir}/*.pdf')\n",
    "    law_docs = []\n",
    "    for filename in filenames:\n",
    "        law_docs.extend(LegalText(filename).documents)\n",
    "    return law_docs\n",
    "\n",
    "def load_qna_data(file_path):\n",
    "    \"\"\"QnA JSON 파일을 읽어서 Document 객체 리스트로 변환합니다.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    for item in data:\n",
    "        metadata = {\n",
    "            **item.get('metadata', {}),\n",
    "            'question': item['question']\n",
    "        }\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=item['answer'],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def create_chroma_db():\n",
    "    \"\"\"Document 객체들로 새로운 Chroma DB를 생성합니다.\"\"\"\n",
    "    DB_PATH = \"./chroma_db\"\n",
    "    PDF_DIR = \"/workspace/LangEyE/App/src/crawling/files/laws\"\n",
    "    JSON_PATH = \"/workspace/LangEyE/App/src/crawling/files/faq_results_20241211.json\"\n",
    "\n",
    "    device = get_device()\n",
    "    \n",
    "    embedding_model = load_embedding(device)\n",
    "    \n",
    "    pdf_documents = load_pdf_documents(PDF_DIR)\n",
    "\n",
    "    qna_documents = load_qna_data(JSON_PATH)\n",
    "    \n",
    "    # 모든 문서 합치기\n",
    "    all_documents = pdf_documents + qna_documents\n",
    "    \n",
    "    db = Chroma.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=DB_PATH,\n",
    "        collection_name=\"my_db\"\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Successfully created Chroma DB with {len(all_documents)} total documents\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QnA 데이터 셋 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_qna_data(file_path):\n",
    "    \"\"\"QnA JSON 파일을 읽어서 Document 객체 리스트로 변환합니다.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    for item in data:\n",
    "        # 메타데이터 결합\n",
    "        metadata = {\n",
    "            **item.get('metadata', {}),\n",
    "            'question': item['question']\n",
    "        }\n",
    "        \n",
    "        # Document 객체 생성\n",
    "        doc = Document(\n",
    "            page_content=item['answer'],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def add_to_chroma(documents, db_path, embedding_model):\n",
    "    \"\"\"Document 객체들을 Chroma DB에 추가합니다.\"\"\"\n",
    "    # 기존 DB에 연결\n",
    "    db = Chroma(\n",
    "        persist_directory=db_path,\n",
    "        embedding_function=embedding_model,\n",
    "        collection_name=\"my_db\"\n",
    "    )\n",
    "    \n",
    "    # 새로운 문서 추가\n",
    "    db.add_documents(documents)\n",
    "    db.persist()\n",
    "    \n",
    "    return db\n",
    "\n",
    "def main():\n",
    "    DB_PATH = \"./chroma_db\"\n",
    "    JSON_PATH = \"/Volumes/MINDB/24년/SW아카데미/LangEyE/App/src/crawling/files/faq_results_20241211.json\"  # JSON 파일 경로\n",
    "    DEVICE = \"cuda\"  # 또는 \"cpu\"\n",
    "    \n",
    "    # 임베딩 모델 로드\n",
    "    embedding_model = load_embedding(DEVICE)\n",
    "    \n",
    "    # QnA 데이터 로드\n",
    "    documents = load_qna_data(JSON_PATH)\n",
    "    \n",
    "    # Chroma DB에 추가\n",
    "    db = add_to_chroma(documents, DB_PATH, embedding_model)\n",
    "    print(f\"Successfully added {len(documents)} documents to Chroma DB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사도 검색\n",
    "\n",
    "`similarity_search` 메서드는 Chroma 데이터베이스에서 유사도 검색을 수행합니다. 이 메서드는 주어진 쿼리와 가장 유사한 문서들을 반환합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `query` (str): 검색할 쿼리 텍스트\n",
    "- `k` (int, 선택적): 반환할 결과의 수. 기본값은 4입니다.\n",
    "- `filter` (Dict[str, str], 선택적): 메타데이터로 필터링. 기본값은 None입니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- `k` 값을 조절하여 원하는 수의 결과를 얻을 수 있습니다.\n",
    "- `filter` 매개변수를 사용하여 특정 메타데이터 조건에 맞는 문서만 검색할 수 있습니다.\n",
    "- 이 메서드는 점수 정보 없이 문서만 반환합니다. 점수 정보도 필요한 경우 `similarity_search_with_score` 메서드를 직접 사용하세요.\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `List[Document]`: 쿼리 텍스트와 가장 유사한 문서들의 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- query 에서 Pattern 을 찾아서 filter 를 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter 사용\n",
    "db.similarity_search(\n",
    "    \"TF IDF 에 대하여 알려줘\", filter={\"source\": \"data/nlp-keywords.txt\"}, k=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "service-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
