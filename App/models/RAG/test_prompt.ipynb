{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt test notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from pdf_preprocessing import LegalText\n",
    "\n",
    "# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "cache_dir = './weights'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
    "os.environ['TORCH_HOME'] = os.path.join(cache_dir, 'torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minsuson/.pyenv/versions/3.10.9/envs/service-env/lib/python3.10/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# ê°€ì¥ ìµœì‹  ë²„ì „ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "def load_model(model_name):\n",
    "    if model_name == 'llama':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"davidkim205/Ko-Llama-3-8B-Instruct\",\n",
    "            cache_dir=cache_dir\n",
    "            )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"davidkim205/Ko-Llama-3-8B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            cache_dir=cache_dir\n",
    "            )\n",
    "    elif model_name == 'qwen': \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"davidkim205/Ko-Qwen-3-8B-Instruct\",\n",
    "            cache_dir=cache_dir\n",
    "            )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"davidkim205/Ko-Qwen-3-8B-Instruct\",\n",
    "            device_map=\"cuda\",\n",
    "            torch_dtype=torch.float16,\n",
    "            cache_dir=cache_dir\n",
    "            )\n",
    "        \n",
    "    return tokenizer, model\n",
    "\n",
    "def load_embedding(model_name, device):\n",
    "    if model_name == 'bge':\n",
    "        model_name = \"upskyy/bge-m3-korean\"\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': device},\n",
    "            encode_kwargs={'normalize_embeddings': True},\n",
    "            cache_folder=cache_dir\n",
    "        )\n",
    "        return embeddings\n",
    "    else:\n",
    "        assert False, f\"Unknown model name: {model_name}\"\n",
    "\n",
    "\n",
    "def load_doc(runpod):\n",
    "    if runpod:\n",
    "        pdf_path = \"/workspace/LangEyE/crawling/ì¥ì• ì¸ë³µì§€ë²•.pdf\"\n",
    "        docs = LegalText(pdf_path).documents\n",
    "    else:\n",
    "        pdf_path = \"/Volumes/MINDB/24á„‚á…§á†«/SWá„‹á…¡á„á…¡á„ƒá…¦á„†á…µ/LangEyE/crawling/ì¥ì• ì¸ë³µì§€ë²•.pdf\"\n",
    "        docs = LegalText(pdf_path).documents\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_634/2991526531.py:37: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2208bc67c34426817217fdb9e8e6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = load_embedding('bge', device)\n",
    "tokenizer, model = load_model('llama')\n",
    "docs = load_doc(True)\n",
    "\n",
    "text_splitter = SemanticChunker(embeddings)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6011/2991526531.py:37: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73ea083c4ec46f7beabcc3e1d13940c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = load_embedding('bge', device)\n",
    "tokenizer, model = load_model('llama')\n",
    "docs = load_doc(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'law_title': '', 'effective_date': '2024-10-22', 'paragraph': 'ì œ5ì¥ ë³µì§€ì‹œì„¤ê³¼ ë‹¨ì²´', 'article_number': 'ì œ59ì¡°ì˜9(ê¸ˆì§€í–‰ìœ„)', 'document_type': 'ë²•ë¥ ', 'is_valid': True, 'legal_area': 'ì¥ì• ì¸ë³µì§€'}, page_content='ëˆ„êµ¬ë“ ì§€ ë‹¤ìŒ ê° í˜¸ì˜ ì–´ëŠ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” í–‰ìœ„ë¥¼ í•˜ì—¬ì„œëŠ” ì•„ë‹ˆ ëœë‹¤. <ê°œì • 2017. 2. 8.>\\n1. ì¥ì• ì¸ì—ê²Œ ì„±ì  ìˆ˜ì¹˜ì‹¬ì„ ì£¼ëŠ” ì„±í¬ë¡±ã†ì„±í­ë ¥ ë“±ì˜ í–‰ìœ„\\n2. ì¥ì• ì¸ì˜ ì‹ ì²´ì— í­í–‰ì„ ê°€í•˜ê±°ë‚˜ ìƒí•´ë¥¼ ì…íˆëŠ” í–‰ìœ„\\n2ì˜2. ì¥ì• ì¸ì„ í­í–‰, í˜‘ë°•, ê°ê¸ˆ, ê·¸ ë°–ì— ì •ì‹ ìƒ ë˜ëŠ” ì‹ ì²´ìƒì˜ ììœ ë¥¼ ë¶€ë‹¹í•˜ê²Œ êµ¬ì†í•˜ëŠ” ìˆ˜ë‹¨ìœ¼ë¡œì¨ ì¥ì• ì¸ì˜ ì\\nìœ ì˜ì‚¬ì— ì–´ê¸‹ë‚˜ëŠ” ë…¸ë™ì„ ê°•ìš”í•˜ëŠ” í–‰ìœ„\\n3. ìì‹ ì˜ ë³´í˜¸ã†ê°ë…ì„ ë°›ëŠ” ì¥ì• ì¸ì„ ìœ ê¸°í•˜ê±°ë‚˜ ì˜ì‹ì£¼ë¥¼ í¬í•¨í•œ ê¸°ë³¸ì  ë³´í˜¸ ë° ì¹˜ë£Œë¥¼ ì†Œí™€íˆ í•˜ëŠ” ë°©ì„í–‰ìœ„\\n4. ì¥ì• ì¸ì—ê²Œ êµ¬ê±¸ì„ í•˜ê²Œ í•˜ê±°ë‚˜ ì¥ì• ì¸ì„ ì´ìš©í•˜ì—¬ êµ¬ê±¸í•˜ëŠ” í–‰ìœ„\\n5. ì¥ì• ì¸ì„ ì²´í¬ ë˜ëŠ” ê°ê¸ˆí•˜ëŠ” í–‰ìœ„\\n6. ì¥ì• ì¸ì˜ ì •ì‹ ê±´ê°• ë° ë°œë‹¬ì— í•´ë¥¼ ë¼ì¹˜ëŠ” ì •ì„œì  í•™ëŒ€í–‰ìœ„\\n7. ì¥ì• ì¸ì„ ìœ„í•˜ì—¬ ì¦ì—¬ ë˜ëŠ” ê¸‰ì—¬ëœ ê¸ˆí’ˆì„ ê·¸ ëª©ì  ì™¸ì˜ ìš©ë„ì— ì‚¬ìš©í•˜ëŠ” í–‰ìœ„\\n8. ê³µì¤‘ì˜ ì˜¤ë½ ë˜ëŠ” í¥í–‰ì„ ëª©ì ìœ¼ë¡œ ì¥ì• ì¸ì˜ ê±´ê°• ë˜ëŠ” ì•ˆì „ì— ìœ í•´í•œ ê³¡ì˜ˆë¥¼ ì‹œí‚¤ëŠ” í–‰ìœ„\\n[ì „ë¬¸ê°œì • 2015. 6. 22.]\\n[ì œ59ì¡°ì˜7ì—ì„œ ì´ë™, ì¢…ì „ ì œ59ì¡°ì˜9ëŠ” ì œ59ì¡°ì˜11ë¡œ ì´ë™ <2017. 12. 19.>]\\nì œ59ì¡°ì˜10(ì¥ì• ì¸í•™ëŒ€ì˜ ì˜ˆë°©ê³¼ ë°©ì§€ ì˜ë¬´) êµ­ê°€ì™€ ì§€ë°©ìì¹˜ë‹¨ì²´ëŠ” ì¥ì• ì¸í•™ëŒ€ì˜ ì˜ˆë°©ê³¼ ë°©ì§€ë¥¼ ìœ„í•˜ì—¬ ë‹¤ìŒ ê° í˜¸ì˜\\nì¡°ì¹˜ë¥¼ ì·¨í•˜ì—¬ì•¼ í•œë‹¤.\\n1. ì¥ì• ì¸í•™ëŒ€ì˜ ì˜ˆë°©ê³¼ ë°©ì§€ë¥¼ ìœ„í•œ ê°ì¢… ì •ì±…ì˜ ìˆ˜ë¦½ ë° ì‹œí–‰\\n2. ì¥ì• ì¸í•™ëŒ€ì˜ ì˜ˆë°©ê³¼ ë°©ì§€ë¥¼ ìœ„í•œ ì—°êµ¬ã†êµìœ¡ã†í™ë³´ì™€ ì¥ì• ì¸í•™ëŒ€ í˜„í™© ì¡°ì‚¬\\n3. ì¥ì• ì¸í•™ëŒ€ì— ê´€í•œ ì‹ ê³ ì²´ê³„ì˜ êµ¬ì¶•ã†ìš´ì˜\\n4. ì¥ì• ì¸í•™ëŒ€ë¡œ ì¸í•˜ì—¬ í”¼í•´ë¥¼ ì…ì€ ì¥ì• ì¸(ì´í•˜ â€œí”¼í•´ì¥ì• ì¸â€ì´ë¼ í•œë‹¤)ì˜ ë³´í˜¸ ë° ì¹˜ë£Œì™€ í”¼í•´ì¥ì• ì¸ì˜ ê°€ì •ì—\\nëŒ€í•œ ì§€ì›\\n5. ì¥ì• ì¸í•™ëŒ€ ì˜ˆë°© ê´€ê³„ ê¸°ê´€ã†ë²•ì¸ã†ë‹¨ì²´ã†ì‹œì„¤ ë“±ì— ëŒ€í•œ ì§€ì›\\n6. ê·¸ ë°–ì— ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ì¥ì• ì¸í•™ëŒ€ì˜ ì˜ˆë°©ê³¼ ë°©ì§€ë¥¼ ìœ„í•œ ì‚¬í•­\\n[ë³¸ì¡°ì‹ ì„¤ 2015. 6. 22.]\\n[ì œ59ì¡°ì˜8ì—ì„œ ì´ë™, ì¢…ì „ ì œ59ì¡°ì˜10ì€ ì œ59ì¡°ì˜12ë¡œ ì´ë™ <2017. 12. 19.>]')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_rag = docs[85:86]\n",
    "temp_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    documents=temp_rag,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 3,  # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ 3ê°œ ê²°ê³¼\n",
    "        \"score_threshold\": 0.5  # ìœ ì‚¬ë„ 0.5 ì´ìƒë§Œ ë°˜í™˜\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¥ì• ì¸ë³µì§€ë²• ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "[ë²•ë ¹ ê¸°ë³¸ ì •ë³´]\n",
      "ë²•ë¥ ëª…: \n",
      "ë¬¸ì„œ ìœ í˜•: \n",
      "ì‹œí–‰ì¼ì: \n",
      "ë²•ë¥  ë¶„ì•¼: \n",
      "\n",
      "[ì°¸ê³ í•  ë²•ë ¹ ë‚´ìš©]\n",
      "\n",
      "\n",
      "[ì§ˆë¬¸]\n",
      "ê¸ˆì§€í–‰ìœ„ì—ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?\n",
      "\n",
      "[ë‹µë³€ ê·œì¹™]\n",
      "1. ìœ„ ë²•ë ¹ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\n",
      "2. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°í•­(ì˜ˆ: ì œ00ì¡° ì œ0í•­)ì„ ë°˜ë“œì‹œ ë¨¼ì € ëª…ì‹œí•©ë‹ˆë‹¤.\n",
      "3. ì¡°í•­ì˜ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•˜ë©´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "4. ë²•ë ¹ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì •í•˜ê±°ë‚˜ í•´ì„í•˜ì§€ ì•Šê³ , \"í•´ë‹¹ ë‚´ìš©ì€ ì œì‹œëœ ë²•ë ¹ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.\n",
      "\n",
      "[ë‹µë³€] \n",
      "ê¸ˆì§€í–‰ìœ„ëŠ” ì¥ì• ì¸ë³µì§€ë²•ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "ì œ1ì¡° (ëª©ì  ë° ë²”ìœ„) ì´ ë²•ì€ ì¥ì• ì¸ì˜ ë³µì§€ë¥¼ ì¦ì§„í•˜ê³  ì¥ì• ì¸ì„ ìœ„í•œ ì‹œì„¤, ì„œë¹„ìŠ¤ ë“±ì„ ë³´ì¥í•¨ìœ¼ë¡œì¨ ì‚¬íšŒí†µí•©ì„ ì‹¤í˜„í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤. ì´ ë²•ì€ ì¥ì• ì¸ì´ ì§ë©´í•˜ëŠ” ì°¨ë³„ê³¼ í˜ì˜¤ë¡œë¶€í„° ë³´í˜¸ë°›ì„ ìˆ˜ ìˆë„ë¡ í•˜ë©°, ì¥ì• ì¸ë“¤ì„ ì¡´ì¤‘í•˜ë©° ê·¸ ê¶Œë¦¬ì™€ ì˜ë¬´ë¥¼ ì¸ì •í•œë‹¤.\n",
      "\n",
      "ë”°ë¼ì„œ, ì¥ì• ì¸ë³µì§€ë²•ì—ì„œëŠ” ì§ì ‘ì ì¸ ê¸ˆì§€í–‰ìœ„ë¥¼ ëª…ì‹œí•˜ê³  ìˆì§€ëŠ” ì•Šì§€ë§Œ, ì¥ì• ì¸ë“¤ì˜ ê¶Œë¦¬ë¥¼ ë³´í˜¸í•˜ê³  ì‚¬íšŒí†µí•©ì„ ìœ„í•´ ë‹¤ì–‘í•œ ì •ì±…ê³¼ ì‹œì„¤ì„ ì œê³µí•˜ëŠ”ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ë§Œì•½ íŠ¹ì • ìƒí™©ì—ì„œ ê¸ˆì§€ë˜ëŠ” í–‰ìœ„ë‚˜ í–‰ë™ì´ ìˆë‹¤ë©´ í•´ë‹¹ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¸ ë²•ë ¹ì´ë‚˜ ê·œì •ì„ ì°¸ê³ í•´ì•¼ í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    if not docs:\n",
    "        return {\n",
    "            \"context\": \"\",\n",
    "            \"law_title\": \"\",\n",
    "            \"document_type\": \"\",\n",
    "            \"effective_date\": \"\",\n",
    "            \"legal_area\": \"\"\n",
    "        }\n",
    "        \n",
    "    formatted_texts = []\n",
    "    first_meta = docs[0].metadata\n",
    "    \n",
    "    for doc in docs:\n",
    "        metadata = doc.metadata\n",
    "        formatted_text = f\"\"\"\n",
    "            [ë²•ë ¹ ìœ„ì¹˜]\n",
    "            ë²•ë¥ ëª…: {metadata['law_title']}\n",
    "            ì¥: {metadata['paragraph'].strip()}\n",
    "            ì¡°ë¬¸: {metadata['article_number'].strip()}\n",
    "            \n",
    "            [ì¡°ë¬¸ ë‚´ìš©]\n",
    "            {doc.page_content.strip()}\n",
    "        \"\"\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "    \n",
    "    return {\n",
    "        \"context\": \"\\n\\n---\\n\\n\".join(formatted_texts),\n",
    "        \"law_title\": first_meta[\"law_title\"],\n",
    "        \"document_type\": first_meta[\"document_type\"],\n",
    "        \"effective_date\": first_meta[\"effective_date\"],\n",
    "        \"legal_area\": first_meta[\"legal_area\"]\n",
    "    }\n",
    "\n",
    "template = \"\"\"ì¥ì• ì¸ë³µì§€ë²• ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "[ë²•ë ¹ ê¸°ë³¸ ì •ë³´]\n",
    "ë²•ë¥ ëª…: {law_title}\n",
    "ë¬¸ì„œ ìœ í˜•: {document_type}\n",
    "ì‹œí–‰ì¼ì: {effective_date}\n",
    "ë²•ë¥  ë¶„ì•¼: {legal_area}\n",
    "\n",
    "[ì°¸ê³ í•  ë²•ë ¹ ë‚´ìš©]\n",
    "{context}\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{question}\n",
    "\n",
    "[ë‹µë³€ ê·œì¹™]\n",
    "1. ìœ„ ë²•ë ¹ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\n",
    "2. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°í•­(ì˜ˆ: ì œ00ì¡° ì œ0í•­)ì„ ë°˜ë“œì‹œ ë¨¼ì € ëª…ì‹œí•©ë‹ˆë‹¤.\n",
    "3. ì¡°í•­ì˜ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•˜ë©´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "4. ë²•ë ¹ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì •í•˜ê±°ë‚˜ í•´ì„í•˜ì§€ ì•Šê³ , \"í•´ë‹¹ ë‚´ìš©ì€ ì œì‹œëœ ë²•ë ¹ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.\n",
    "\n",
    "[ë‹µë³€]\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def combine_metadata(retrieved_docs, original_query):\n",
    "    doc_metadata = retrieved_docs\n",
    "    return {\n",
    "        \"law_title\": doc_metadata[\"law_title\"],\n",
    "        \"document_type\": doc_metadata[\"document_type\"],\n",
    "        \"effective_date\": doc_metadata[\"effective_date\"],\n",
    "        \"legal_area\": doc_metadata[\"legal_area\"],\n",
    "        \"context\": doc_metadata[\"context\"],\n",
    "        \"question\": original_query\n",
    "    }\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„± ìˆ˜ì •\n",
    "retrieval_chain = {\n",
    "    \"original_query\": RunnablePassthrough(),\n",
    "    \"retrieved_docs\": retriever | RunnableLambda(format_docs)\n",
    "}\n",
    "\n",
    "# RAG ì²´ì¸ ìƒì„± (ì‚¬ìš©ì ì •ë³´ í¬í•¨)\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": get_context,\n",
    "        \"question\": lambda x: x if isinstance(x, str) else x[\"question\"]\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "test_question = \"ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜9ì—ëŠ” ë¬´ì—‡ì´ ëª…ì‹œë˜ì–´ ìˆë‚˜ìš”?\"\n",
    "answer = rag_chain.invoke({\"question\": test_question})\n",
    "print(test_question)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ ì§ˆë¬¸: ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜9ì—ëŠ” ë¬´ì—‡ì´ ëª…ì‹œë˜ì–´ ìˆë‚˜ìš”?\n",
      "\n",
      "================================================================================\n",
      "ğŸ“š ì°¸ì¡° ì¡°í•­:\n",
      "  â€¢ ì œ11ì¡°\n",
      "  â€¢ ì œ00ì¡° ì œ0í•­\n",
      "  â€¢ ì œ59ì¡°\n",
      "\n",
      "ğŸ“Œ ë‹µë³€ ìš”ì•½:\n",
      "  ì¥ì• ì¸ë³µì§€ë²• ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“ ìƒì„¸ ì„¤ëª…:\n",
      "\n",
      "ì°¸ê³ í•  ë²•ë ¹ ë‚´ìš©:\n",
      "9.>\n",
      "1. ì¥ì• ì¸ë³µì§€ì •ì±…ì˜ ê¸°ë³¸ë°©í–¥ì— ê´€í•œ ì‚¬í•­\n",
      "2. ì¥ì• ì¸ë³µì§€ í–¥ìƒì„ ìœ„í•œ ì œë„ê°œì„ ê³¼ ì˜ˆì‚°ì§€ì›ì— ê´€í•œ ì‚¬í•­\n",
      "3. ì¤‘ìš”í•œ íŠ¹ìˆ˜êµìœ¡ì •ì±…ì˜ ì¡°ì •ì— ê´€í•œ ì‚¬í•­\n",
      "4. ì¥ì• ì¸ ê³ ìš©ì´‰ì§„ì •ì±…ì˜ ì¤‘ìš”í•œ ì¡°ì •ì— ê´€í•œ ì‚¬í•­\n",
      "5. ì¥ì• ì¸ ì´ë™ë³´ì¥ ì •ì±…ì¡°ì •ì— ê´€í•œ ì‚¬í•­\n",
      "6. ì¥ì• ì¸ì •ì±… ì¶”ì§„ê³¼ ê´€ë ¨í•œ ì¬ì›ì¡°ë‹¬ì— ê´€í•œ ì‚¬í•­\n",
      "7. ì¥ì• ì¸ë³µì§€ì— ê´€í•œ ê´€ë ¨ ë¶€ì²˜ì˜ í˜‘ì¡°ì— ê´€í•œ ì‚¬í•­\n",
      "7ì˜2. ë‹¤ë¥¸ ë²•ë ¹ì—ì„œ ìœ„ì›íšŒì˜ ì‹¬ì˜ë¥¼ ê±°ì¹˜ë„ë¡ í•œ ì‚¬í•­\n",
      "8. ê·¸ ë°–ì— ì¥ì• ì¸ë³µì§€ì™€ ê´€ë ¨í•˜ì—¬ ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ì‚¬í•­\n",
      "â‘¢ìœ„ì›íšŒëŠ” í•„ìš”í•˜ë‹¤ê³  ì¸ì •ë˜ë©´ ê´€ê³„ í–‰ì •ê¸°ê´€ì— ê·¸ ì§ì›ì˜ ì¶œì„ã†ì„¤ëª…ê³¼ ìë£Œ ì œì¶œì„ ìš”êµ¬í•  ìˆ˜ ìˆë‹¤. â‘£ìœ„ì›íšŒëŠ” ì œ2í•­ì˜ ì‚¬í•­ì„ ë¯¸ë¦¬ ê²€í† í•˜ê³  ê´€ê³„ ê¸°ê´€ ì‚¬ì´ì˜ í˜‘ì¡° ì‚¬í•­ì„ ì •ë¦¬í•˜ê¸° ìœ„í•˜ì—¬ ìœ„ì›íšŒì— ì¥ì• ì¸ì •ì±…ì¡°\n",
      "ì •ì‹¤ë¬´ìœ„ì›íšŒ(ì´í•˜ â€œì‹¤ë¬´ìœ„ì›íšŒâ€ë¼ í•œë‹¤)ë¥¼ ë‘”ë‹¤. â‘¤ìœ„ì›íšŒì™€ ì‹¤ë¬´ìœ„ì›íšŒì˜ êµ¬ì„±ã†ìš´ì˜ì— ê´€í•˜ì—¬ í•„ìš”í•œ ì‚¬í•­ì€ ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•œë‹¤.\n",
      "\n",
      "30., 2019. 1. 15.>\n",
      "â‘¥ì œ2í•­ì— ë”°ë¥¸ ì¥ì• ì¸ë³µì§€ì‹œì„¤ì˜ ì‹œì„¤ê¸°ì¤€ã†ì‹ ê³ ã†ë³€ê²½ì‹ ê³  ë° ì´ìš© ë“±ì— ê´€í•˜ì—¬ í•„ìš”í•œ ì‚¬í•­ì€ ë³´ê±´ë³µì§€ë¶€ë ¹ìœ¼\n",
      "ë¡œ ì •í•œë‹¤.<ê°œì • 2008. 2. 29., 2010. 1. 18., 2011. 3. 30., 2019. 1. 15.>\n",
      " \n",
      "\n",
      "â‘  ì œ59ì¡°ì˜4ì— ë”°ë¼ ì¥ì• ì¸í•™ëŒ€ ì‹ ê³ ë¥¼ ì ‘ìˆ˜í•œ ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ì˜ ì§ì›ì´ë‚˜ ì‚¬ë²•ê²½\n",
      "ì°°ê´€ë¦¬ëŠ” ì§€ì²´ ì—†ì´ ì¥ì• ì¸í•™ëŒ€í˜„ì¥ì— ì¶œë™í•˜ì—¬ì•¼ í•œë‹¤. ì´ ê²½ìš° ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ì˜ ì¥ì´ë‚˜ ìˆ˜ì‚¬ê¸°ê´€ì˜ ì¥ì€ ì„œ\n",
      "ë¡œ ë™í–‰í•˜ì—¬ ì¤„ ê²ƒì„ ìš”ì²­í•  ìˆ˜ ìˆìœ¼ë©°, ê·¸ ìš”ì²­ì„ ë°›ì€ ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ì˜ ì¥ì´ë‚˜ ìˆ˜ì‚¬ê¸°ê´€ì˜ ì¥ì€ ì •ë‹¹í•œ ì‚¬ìœ \n",
      "ê°€ ì—†ìœ¼ë©´ ì†Œì† ì§ì›ì´ë‚˜ ì‚¬ë²•ê²½ì°°ê´€ë¦¬ê°€ í˜„ì¥ì— ë™í–‰í•˜ë„ë¡ í•˜ì—¬ì•¼ í•œë‹¤. <ê°œì • 2015.\n",
      "\n",
      "â‘  ëˆ„êµ¬ë“ ì§€ ì¥ì• ì¸í•™ëŒ€ ë° ì¥ì• ì¸ ëŒ€ìƒ ì„±ë²”ì£„ë¥¼ ì•Œ\n",
      "ê²Œ ëœ ë•Œì—ëŠ” ì œ59ì¡°ì˜11ì— ë”°ë¥¸ ì¤‘ì•™ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ ë˜ëŠ” ì§€ì—­ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€(ì´í•˜ â€œì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€\n",
      "â€ì´ë¼ í•œë‹¤)ì´ë‚˜ ìˆ˜ì‚¬ê¸°ê´€ì— ì‹ ê³ í•  ìˆ˜ ìˆë‹¤. <ê°œì • 2015. 6. 22., 2015. 12. 29., 2017. 12. 19.>\n",
      "â‘¡ ë‹¤ìŒ ê° í˜¸ì˜ ì–´ëŠ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” ì‚¬ëŒì€ ê·¸ ì§ë¬´ìƒ ì¥ì• ì¸í•™ëŒ€ ë° ì¥ì• ì¸ ëŒ€ìƒ ì„±ë²”ì£„ë¥¼ ì•Œê²Œ ëœ ê²½ìš°ì—ëŠ” ì§€\n",
      "ì²´ ì—†ì´ ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ ë˜ëŠ” ìˆ˜ì‚¬ê¸°ê´€ì— ì‹ ê³ í•˜ì—¬ì•¼ í•œë‹¤.<ê°œì • 2015. 6.\n",
      "\n",
      "ì§ˆë¬¸: ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜9ì—ëŠ” ë¬´ì—‡ì´ ëª…ì‹œë˜ì–´ ìˆë‚˜ìš”?\n",
      "\n",
      "ë‹µë³€ ê·œì¹™:\n",
      "1. ìœ„ ë²•ë ¹ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\n",
      "2. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°í•­(ì˜ˆ: ì œ00ì¡° ì œ0í•­)ì„ ë°˜ë“œì‹œ ë¨¼ì € ëª…ì‹œí•©ë‹ˆë‹¤.\n",
      "3. ì¡°í•­ì˜ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•˜ë©´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "4. ë²•ë ¹ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì •í•˜ê±°ë‚˜ í•´ì„í•˜ì§€ ì•Šê³ , \"í•´ë‹¹ ë‚´ìš©ì€ ì œì‹œëœ ë²•ë ¹ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.\n",
      "\n",
      "ë‹µë³€: ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜9ëŠ” í˜„ì¬ ì‚­ì œë˜ê³  ìˆìŠµë‹ˆë‹¤. (ì‚­ì œëœ ì¡°í•­) <2015. 12. 29.> ë”°ë¼ì„œ í•´ë‹¹ ë‚´ìš©ì€ ì œì‹œëœ ë²•ë ¹ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. \n",
      "\n",
      "ì§ˆë¬¸: ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜4ì—ì„œëŠ” ì–´ë–¤ ìƒí™©ì—ì„œ ì¥ì• ì¸ í•™ëŒ€ ë° ì¥ì• ì¸ ëŒ€ìƒ ì„±ë²”ì£„ë¥¼ ì‹ ê³ í•´ì•¼ í•˜ëŠ”ì§€ ëª…ì‹œë˜ì–´ ìˆë‚˜ìš”?\n",
      "\n",
      "ë‹µë³€: ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜4ì—ì„œëŠ” ëˆ„êµ¬ë“ ì§€ ì¥ì• ì¸ í•™ëŒ€ ë° ì¥ì• ì¸ ëŒ€ìƒ ì„±ë²”ì£„ë¥¼ ì•Œê³  ìˆì„ ë•Œ, ì¦‰ì‹œ ì¤‘ì•™ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ì´ë‚˜ ì§€ì—­ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ ë˜ëŠ” ìˆ˜ì‚¬ê¸°ê´€ì— ì‹ ê³ í•´ì•¼ í•œë‹¤ëŠ” ì±…ì„ì´ ì£¼ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. <ê°œì • 2015. 6. 22., 2015. 12. 29., 2017. 12. 19.> \n",
      "\n",
      "ì§ˆë¬¸: ì¥ì• ì¸ë³µì§€ë²• ì œ11ì¡°ì— ë”°ë¥´ë©´ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•©ë‹ˆë‹¤?\n",
      "\n",
      "ë‹µë³€: ì¥ì• ì¸ë³µì§€ë²• ì œ11ì¡°ëŠ” ì¥ì• ì¸ì •ì±…ì¡°ì •ìœ„ì›íšŒë¥¼ ì„¤ë¦½í•˜ê³  ê·¸ ì„ê¸°\n",
      "\n",
      "âš ï¸ ì£¼ì˜ì‚¬í•­:\n",
      "  â€¢ ì¼ë¶€ ë‚´ìš©ì€ ì œì‹œëœ ë²•ë ¹ì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'references': ['ì œ11ì¡°', 'ì œ00ì¡° ì œ0í•­', 'ì œ59ì¡°'],\n",
       " 'summary': 'ì¥ì• ì¸ë³µì§€ë²• ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.',\n",
       " 'details': '\\nì°¸ê³ í•  ë²•ë ¹ ë‚´ìš©:\\n9.>\\n1. ì¥ì• ì¸ë³µì§€ì •ì±…ì˜ ê¸°ë³¸ë°©í–¥ì— ê´€í•œ ì‚¬í•­\\n2. ì¥ì• ì¸ë³µì§€ í–¥ìƒì„ ìœ„í•œ ì œë„ê°œì„ ê³¼ ì˜ˆì‚°ì§€ì›ì— ê´€í•œ ì‚¬í•­\\n3. ì¤‘ìš”í•œ íŠ¹ìˆ˜êµìœ¡ì •ì±…ì˜ ì¡°ì •ì— ê´€í•œ ì‚¬í•­\\n4. ì¥ì• ì¸ ê³ ìš©ì´‰ì§„ì •ì±…ì˜ ì¤‘ìš”í•œ ì¡°ì •ì— ê´€í•œ ì‚¬í•­\\n5. ì¥ì• ì¸ ì´ë™ë³´ì¥ ì •ì±…ì¡°ì •ì— ê´€í•œ ì‚¬í•­\\n6. ì¥ì• ì¸ì •ì±… ì¶”ì§„ê³¼ ê´€ë ¨í•œ ì¬ì›ì¡°ë‹¬ì— ê´€í•œ ì‚¬í•­\\n7. ì¥ì• ì¸ë³µì§€ì— ê´€í•œ ê´€ë ¨ ë¶€ì²˜ì˜ í˜‘ì¡°ì— ê´€í•œ ì‚¬í•­\\n7ì˜2. ë‹¤ë¥¸ ë²•ë ¹ì—ì„œ ìœ„ì›íšŒì˜ ì‹¬ì˜ë¥¼ ê±°ì¹˜ë„ë¡ í•œ ì‚¬í•­\\n8. ê·¸ ë°–ì— ì¥ì• ì¸ë³µì§€ì™€ ê´€ë ¨í•˜ì—¬ ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ì‚¬í•­\\nâ‘¢ìœ„ì›íšŒëŠ” í•„ìš”í•˜ë‹¤ê³  ì¸ì •ë˜ë©´ ê´€ê³„ í–‰ì •ê¸°ê´€ì— ê·¸ ì§ì›ì˜ ì¶œì„ã†ì„¤ëª…ê³¼ ìë£Œ ì œì¶œì„ ìš”êµ¬í•  ìˆ˜ ìˆë‹¤. â‘£ìœ„ì›íšŒëŠ” ì œ2í•­ì˜ ì‚¬í•­ì„ ë¯¸ë¦¬ ê²€í† í•˜ê³  ê´€ê³„ ê¸°ê´€ ì‚¬ì´ì˜ í˜‘ì¡° ì‚¬í•­ì„ ì •ë¦¬í•˜ê¸° ìœ„í•˜ì—¬ ìœ„ì›íšŒì— ì¥ì• ì¸ì •ì±…ì¡°\\nì •ì‹¤ë¬´ìœ„ì›íšŒ(ì´í•˜ â€œì‹¤ë¬´ìœ„ì›íšŒâ€ë¼ í•œë‹¤)ë¥¼ ë‘”ë‹¤. â‘¤ìœ„ì›íšŒì™€ ì‹¤ë¬´ìœ„ì›íšŒì˜ êµ¬ì„±ã†ìš´ì˜ì— ê´€í•˜ì—¬ í•„ìš”í•œ ì‚¬í•­ì€ ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•œë‹¤.\\n\\n30., 2019. 1. 15.>\\nâ‘¥ì œ2í•­ì— ë”°ë¥¸ ì¥ì• ì¸ë³µì§€ì‹œì„¤ì˜ ì‹œì„¤ê¸°ì¤€ã†ì‹ ê³ ã†ë³€ê²½ì‹ ê³  ë° ì´ìš© ë“±ì— ê´€í•˜ì—¬ í•„ìš”í•œ ì‚¬í•­ì€ ë³´ê±´ë³µì§€ë¶€ë ¹ìœ¼\\në¡œ ì •í•œë‹¤.<ê°œì • 2008. 2. 29., 2010. 1. 18., 2011. 3. 30., 2019. 1. 15.>\\n \\n\\nâ‘  ì œ59ì¡°ì˜4ì— ë”°ë¼ ì¥ì• ì¸í•™ëŒ€ ì‹ ê³ ë¥¼ ì ‘ìˆ˜í•œ ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ì˜ ì§ì›ì´ë‚˜ ì‚¬ë²•ê²½\\nì°°ê´€ë¦¬ëŠ” ì§€ì²´ ì—†ì´ ì¥ì• ì¸í•™ëŒ€í˜„ì¥ì— ì¶œë™í•˜ì—¬ì•¼ í•œë‹¤. ì´ ê²½ìš° ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ì˜ ì¥ì´ë‚˜ ìˆ˜ì‚¬ê¸°ê´€ì˜ ì¥ì€ ì„œ\\në¡œ ë™í–‰í•˜ì—¬ ì¤„ ê²ƒì„ ìš”ì²­í•  ìˆ˜ ìˆìœ¼ë©°, ê·¸ ìš”ì²­ì„ ë°›ì€ ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ì˜ ì¥ì´ë‚˜ ìˆ˜ì‚¬ê¸°ê´€ì˜ ì¥ì€ ì •ë‹¹í•œ ì‚¬ìœ \\nê°€ ì—†ìœ¼ë©´ ì†Œì† ì§ì›ì´ë‚˜ ì‚¬ë²•ê²½ì°°ê´€ë¦¬ê°€ í˜„ì¥ì— ë™í–‰í•˜ë„ë¡ í•˜ì—¬ì•¼ í•œë‹¤. <ê°œì • 2015.\\n\\nâ‘  ëˆ„êµ¬ë“ ì§€ ì¥ì• ì¸í•™ëŒ€ ë° ì¥ì• ì¸ ëŒ€ìƒ ì„±ë²”ì£„ë¥¼ ì•Œ\\nê²Œ ëœ ë•Œì—ëŠ” ì œ59ì¡°ì˜11ì— ë”°ë¥¸ ì¤‘ì•™ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ ë˜ëŠ” ì§€ì—­ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€(ì´í•˜ â€œì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€\\nâ€ì´ë¼ í•œë‹¤)ì´ë‚˜ ìˆ˜ì‚¬ê¸°ê´€ì— ì‹ ê³ í•  ìˆ˜ ìˆë‹¤. <ê°œì • 2015. 6. 22., 2015. 12. 29., 2017. 12. 19.>\\nâ‘¡ ë‹¤ìŒ ê° í˜¸ì˜ ì–´ëŠ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” ì‚¬ëŒì€ ê·¸ ì§ë¬´ìƒ ì¥ì• ì¸í•™ëŒ€ ë° ì¥ì• ì¸ ëŒ€ìƒ ì„±ë²”ì£„ë¥¼ ì•Œê²Œ ëœ ê²½ìš°ì—ëŠ” ì§€\\nì²´ ì—†ì´ ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ ë˜ëŠ” ìˆ˜ì‚¬ê¸°ê´€ì— ì‹ ê³ í•˜ì—¬ì•¼ í•œë‹¤.<ê°œì • 2015. 6.\\n\\nì§ˆë¬¸: ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜9ì—ëŠ” ë¬´ì—‡ì´ ëª…ì‹œë˜ì–´ ìˆë‚˜ìš”?\\n\\në‹µë³€ ê·œì¹™:\\n1. ìœ„ ë²•ë ¹ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\\n2. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°í•­(ì˜ˆ: ì œ00ì¡° ì œ0í•­)ì„ ë°˜ë“œì‹œ ë¨¼ì € ëª…ì‹œí•©ë‹ˆë‹¤.\\n3. ì¡°í•­ì˜ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•˜ë©´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.\\n4. ë²•ë ¹ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì •í•˜ê±°ë‚˜ í•´ì„í•˜ì§€ ì•Šê³ , \"í•´ë‹¹ ë‚´ìš©ì€ ì œì‹œëœ ë²•ë ¹ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.\\n\\në‹µë³€: ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜9ëŠ” í˜„ì¬ ì‚­ì œë˜ê³  ìˆìŠµë‹ˆë‹¤. (ì‚­ì œëœ ì¡°í•­) <2015. 12. 29.> ë”°ë¼ì„œ í•´ë‹¹ ë‚´ìš©ì€ ì œì‹œëœ ë²•ë ¹ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. \\n\\nì§ˆë¬¸: ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜4ì—ì„œëŠ” ì–´ë–¤ ìƒí™©ì—ì„œ ì¥ì• ì¸ í•™ëŒ€ ë° ì¥ì• ì¸ ëŒ€ìƒ ì„±ë²”ì£„ë¥¼ ì‹ ê³ í•´ì•¼ í•˜ëŠ”ì§€ ëª…ì‹œë˜ì–´ ìˆë‚˜ìš”?\\n\\në‹µë³€: ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜4ì—ì„œëŠ” ëˆ„êµ¬ë“ ì§€ ì¥ì• ì¸ í•™ëŒ€ ë° ì¥ì• ì¸ ëŒ€ìƒ ì„±ë²”ì£„ë¥¼ ì•Œê³  ìˆì„ ë•Œ, ì¦‰ì‹œ ì¤‘ì•™ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ì´ë‚˜ ì§€ì—­ì¥ì• ì¸ê¶Œìµì˜¹í˜¸ê¸°ê´€ ë˜ëŠ” ìˆ˜ì‚¬ê¸°ê´€ì— ì‹ ê³ í•´ì•¼ í•œë‹¤ëŠ” ì±…ì„ì´ ì£¼ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. <ê°œì • 2015. 6. 22., 2015. 12. 29., 2017. 12. 19.> \\n\\nì§ˆë¬¸: ì¥ì• ì¸ë³µì§€ë²• ì œ11ì¡°ì— ë”°ë¥´ë©´ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•©ë‹ˆë‹¤?\\n\\në‹µë³€: ì¥ì• ì¸ë³µì§€ë²• ì œ11ì¡°ëŠ” ì¥ì• ì¸ì •ì±…ì¡°ì •ìœ„ì›íšŒë¥¼ ì„¤ë¦½í•˜ê³  ê·¸ ì„ê¸°',\n",
       " 'limitations': ['ì¼ë¶€ ë‚´ìš©ì€ ì œì‹œëœ ë²•ë ¹ì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì¶œë ¥ í¬ë§·íŒ…ì„ ìœ„í•œ í•¨ìˆ˜ë“¤\n",
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "def format_legal_response(response: str) -> Dict:\n",
    "    \"\"\"\n",
    "    ë²•ë¥  ë‹µë³€ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'references': [ì°¸ì¡° ì¡°í•­ë“¤],\n",
    "            'summary': ìš”ì•½ ë‹µë³€,\n",
    "            'details': ìƒì„¸ ì„¤ëª…,\n",
    "            'limitations': í•œê³„ì ì´ë‚˜ ì£¼ì˜ì‚¬í•­\n",
    "        }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ì°¸ì¡° ì¡°í•­ ì¶”ì¶œ (ì˜ˆ: ì œ00ì¡° ì œ0í•­, ì œ0í˜¸ ë“±)\n",
    "        references = re.findall(r'ì œ\\d+ì¡°(?:\\s*ì œ\\d+í•­)?(?:\\s*ì œ\\d+í˜¸)?', response)\n",
    "        \n",
    "        # ë‹µë³€ ë³¸ë¬¸ì—ì„œ ì°¸ì¡° ì¡°í•­ ì œì™¸í•œ ì‹¤ì œ ì„¤ëª… ë¶€ë¶„ ì¶”ì¶œ\n",
    "        details = response.split('\\n')\n",
    "        main_content = [line for line in details if not line.startswith('ì œ')]\n",
    "        \n",
    "        # í•œê³„ì ì´ë‚˜ ì£¼ì˜ì‚¬í•­ í™•ì¸ (ë²•ë ¹ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš© ë“±)\n",
    "        limitations = []\n",
    "        if \"ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤\" in response:\n",
    "            limitations.append(\"ì¼ë¶€ ë‚´ìš©ì€ ì œì‹œëœ ë²•ë ¹ì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        return {\n",
    "            'references': list(set(references)),  # ì¤‘ë³µ ì œê±°\n",
    "            'summary': main_content[0] if main_content else \"\",\n",
    "            'details': '\\n'.join(main_content[1:]) if len(main_content) > 1 else \"\",\n",
    "            'limitations': limitations\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': f\"ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\",\n",
    "            'raw_response': response\n",
    "        }\n",
    "\n",
    "def print_formatted_response(parsed_response: Dict):\n",
    "    \"\"\"\n",
    "    íŒŒì‹±ëœ ì‘ë‹µì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ“š ì°¸ì¡° ì¡°í•­:\")\n",
    "    if parsed_response.get('references'):\n",
    "        for ref in parsed_response['references']:\n",
    "            print(f\"  â€¢ {ref}\")\n",
    "    else:\n",
    "        print(\"  â€¢ ëª…ì‹œì  ì°¸ì¡° ì¡°í•­ ì—†ìŒ\")\n",
    "    \n",
    "    print(\"\\nğŸ“Œ ë‹µë³€ ìš”ì•½:\")\n",
    "    print(f\"  {parsed_response.get('summary', 'ìš”ì•½ ì—†ìŒ')}\")\n",
    "    \n",
    "    if parsed_response.get('details'):\n",
    "        print(\"\\nğŸ“ ìƒì„¸ ì„¤ëª…:\")\n",
    "        print(parsed_response['details'])\n",
    "    \n",
    "    if parsed_response.get('limitations'):\n",
    "        print(\"\\nâš ï¸ ì£¼ì˜ì‚¬í•­:\")\n",
    "        for limitation in parsed_response['limitations']:\n",
    "            print(f\"  â€¢ {limitation}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# RAG ì²´ì¸ ìˆ˜ì •\n",
    "def process_rag_response(question: str, response: str):\n",
    "    \"\"\"\n",
    "    RAG ì²´ì¸ì˜ ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³  í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    parsed = format_legal_response(response)\n",
    "    print(f\"â“ ì§ˆë¬¸: {question}\\n\")\n",
    "    print_formatted_response(parsed)\n",
    "    return parsed\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "test_question = \"ì¥ì• ì¸ë³µì§€ë²• ì œ59ì¡°ì˜9ì—ëŠ” ë¬´ì—‡ì´ ëª…ì‹œë˜ì–´ ìˆë‚˜ìš”?\"\n",
    "answer = rag_chain.invoke({\"question\": test_question})\n",
    "process_rag_response(test_question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "service-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
