{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt test notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "from load_embedding import load_embedding\n",
    "from set_db import create_chroma_db\n",
    "from load_llm import load_model\n",
    "\n",
    "# 캐시 디렉토리 설정\n",
    "cache_dir = './weights'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
    "os.environ['TORCH_HOME'] = os.path.join(cache_dir, 'torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalQASystem:\n",
    "    def __init__(self, prompt_file, model_name, cache_dir='./weights'):\n",
    "        \"\"\"prompt/qna.yaml, prompt/law.yaml\"\"\"\n",
    "        self.load_prompt(prompt_file)\n",
    "        self.setup_qa_system()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.llm = load_model('llama') # or qwen\n",
    "    \n",
    "    def load_prompt(self, prompt_file):\n",
    "        with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "            self.prompts = yaml.safe_load(f)\n",
    "    \n",
    "    def setup_qa_system(self):\n",
    "        embeddings = load_embedding()\n",
    "\n",
    "        ## Set DB\n",
    "        self.db = create_chroma_db(embeddings)\n",
    "\n",
    "        qa_template = PromptTemplate(\n",
    "            input_variables=['context','chat_history','question'],\n",
    "            template = self.prompts['qa_template']\n",
    "        )\n",
    "\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key='chat_history',\n",
    "            return_messages=True,\n",
    "        )\n",
    "\n",
    "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            retriever=self.db.as_retriever(search_kwargs={'k' 5}),\n",
    "            memory=self.memory,\n",
    "            combine_docs_chain_kwargs={'prompt':qa_template},\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "    \n",
    "    def get_answer(self, question):\n",
    "        \"\"\"질문에 대한 답변 생성\"\"\"\n",
    "        try:\n",
    "            result = self.qa_chain({'question': question})\n",
    "\n",
    "            sources =[]\n",
    "            for doc in result['source_documents']:\n",
    "                citation = self.prompts['citation_template'].format(\n",
    "                    law_name=doc.metadata.get('law_name', '정보 없음'),\n",
    "                    paragraph=doc.metadata.get('paragraph', '정보 없음'),\n",
    "                    article_number=doc.metadata.get('article_number', '정보 없음'),\n",
    "                    effective_date=doc.metadata.get('effective_date', '정보 없음'),\n",
    "                )\n",
    "                sources.append(citation)\n",
    "            \n",
    "            return {\n",
    "                'status' : 'success',\n",
    "                'answer' : result['answer'],\n",
    "                'sources' : sources\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status' : 'error',\n",
    "                'answer' : self.prompts['error_message'],\n",
    "                'error' : str(e)\n",
    "            }\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    qa_system = LegalQASystem(prompt_file='prompt/qna.yaml', model_name = 'llama')\n",
    "\n",
    "    question = '시각장애인 보조견 동반 출입 거부 시 처벌 규정이 있나요?'\n",
    "    response = qa_system.get_answer(question)\n",
    "\n",
    "    print('답변: ', response['answer'])\n",
    "    print('\\n참고문서:')\n",
    "    for source in response['sources']:\n",
    "        print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template 읽어오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['fruit'], input_types={}, partial_variables={}, template='{fruit}의 색깔이 뭐야?')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt('prompt/law.yaml', encoding= 'utf-8')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "\n",
    "- 대화목록을 프롬프트로 주입하고자 할 때 활용할 수 있다.\n",
    "- 메세지는 튜플로 구성, (role, message) 형태로 구성한다.\n",
    "- role\n",
    "    - \"system\"\n",
    "    - \"human\"\n",
    "    - \"ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['name', 'user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 {name} 입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='반가워요!'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='안녕하세요! 무엇을 도와드릴까요?'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # role, message\n",
    "        (\"system\", \"당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 {name} 입니다.\"),\n",
    "        (\"human\", \"반가워요!\"),\n",
    "        (\"ai\", \"안녕하세요! 무엇을 도와드릴까요?\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MessagePlaceholder\n",
    "\n",
    "또한 LangChain은 포맷하는 동안 렌더링할 메시지를 완전히 제어할 수 있는 `MessagePlaceholder` 를 제공합니다. \n",
    "\n",
    "메시지 프롬프트 템플릿에 어떤 역할을 사용해야 할지 확실하지 않거나 서식 지정 중에 메시지 목록을 삽입하려는 경우 유용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 요약 전문 AI 어시스턴트입니다. 당신의 임무는 주요 키워드로 대화를 요약하는 것입니다.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "        (\"human\", \"지금까지의 대화를 {word_count} 단어로 요약합니다.\"),\n",
    "    ]\n",
    ")\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_chat_prompt = chat_prompt.format(\n",
    "    word_count=5,\n",
    "    conversation=[\n",
    "        (\"human\", \"안녕하세요! 저는 오늘 새로 입사한 테디 입니다. 만나서 반갑습니다.\"),\n",
    "        (\"ai\", \"반가워요! 앞으로 잘 부탁 드립니다.\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_embedding import load_embedding\n",
    "\n",
    "# 모델 로드\n",
    "embedding = load_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from set_db import create_chroma_db\n",
    "\n",
    "db = create_chroma_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "def load_model(model_name):\n",
    "    if model_name == 'llama':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"davidkim205/Ko-Llama-3-8B-Instruct\",\n",
    "            cache_dir=cache_dir\n",
    "            )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"davidkim205/Ko-Llama-3-8B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            cache_dir=cache_dir\n",
    "            )\n",
    "    elif model_name == 'qwen': \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"davidkim205/Ko-Qwen-3-8B-Instruct\",\n",
    "            cache_dir=cache_dir\n",
    "            )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"davidkim205/Ko-Qwen-3-8B-Instruct\",\n",
    "            device_map=\"cuda\",\n",
    "            torch_dtype=torch.float16,\n",
    "            cache_dir=cache_dir\n",
    "            )\n",
    "        \n",
    "    return tokenizer, model\n",
    "\n",
    "def load_embedding(model_name, device):\n",
    "    if model_name == 'bge':\n",
    "        model_name = \"upskyy/bge-m3-korean\"\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': device},\n",
    "            encode_kwargs={'normalize_embeddings': True},\n",
    "            cache_folder=cache_dir\n",
    "        )\n",
    "        return embeddings\n",
    "    else:\n",
    "        assert False, f\"Unknown model name: {model_name}\"\n",
    "\n",
    "\n",
    "def load_doc(runpod):\n",
    "    if runpod:\n",
    "        pdf_path = \"/workspace/LangEyE/crawling/장애인복지법.pdf\"\n",
    "        docs = LegalText(pdf_path).documents\n",
    "    else:\n",
    "        pdf_path = \"/Volumes/MINDB/24년/SW아카데미/LangEyE/crawling/장애인복지법.pdf\"\n",
    "        docs = LegalText(pdf_path).documents\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_530/2991526531.py:37: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d3c3e079264b75bc2eb502a5bd2389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = load_embedding('bge', device)\n",
    "tokenizer, model = load_model('llama')\n",
    "docs = load_doc(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum Marginal Relevance 사용\n",
    "    search_kwargs={\n",
    "        \"k\": 4,  # 검색 결과 수 증가\n",
    "        \"fetch_k\": 20,  # candidate 검색 수 증가\n",
    "        \"lambda_mult\": 0.7,  # diversity vs similarity 조절\n",
    "        \"filter\": lambda x: True  # 기본 필터 설정\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 장애인복지법 제59조의7에는 무엇이 명시되어 있나요?\n",
      "답변: 장애인복지법 관련 질의응답을 진행하겠습니다.\n",
      "[법령 기본 정보]\n",
      "법률명: \n",
      "문서 유형: 법률\n",
      "시행일자: 2024-10-22\n",
      "법률 분야: 장애인복지\n",
      "\n",
      "[참고할 법령 내용]\n",
      "\n",
      "            [법령 위치]\n",
      "            법률명: \n",
      "            장: 제5장 복지시설과 단체\n",
      "            조문: 제59조의6\n",
      "            \n",
      "            [조문 내용]\n",
      "            장애인학대 및 장애인 대상 성범죄 신고인에 대하여는 「특정범죄\n",
      "신고자 등 보호법」 제7조부터 제13조까지의 규정을 준용한다.\n",
      "[본조신설 2017. 12. 19.]\n",
      "[종전 제59조의6은 제59조의8로 이동 <2017. 12. 19.>]\n",
      "        \n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "            [법령 위치]\n",
      "            법률명: \n",
      "            장: 제1장 총칙\n",
      "            조문: 제3조\n",
      "            \n",
      "            [조문 내용]\n",
      "            장애인복지의 기본이념은 장애인의 완전한 사회 참여와 평등을 통하여 사회통합을 이루는 데에 있다.\n",
      "        \n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "            [법령 위치]\n",
      "            법률명: \n",
      "            장: 제3장 복지 조치\n",
      "            조문: 제44조\n",
      "            \n",
      "            [조문 내용]\n",
      "            국가, 지방자치단체 및 그 밖의 공공단체는 장애인복지시설과 장애인복지단체에서 생산한 물품의\n",
      "우선 구매에 필요한 조치를 마련하여야 한다.\n",
      "[전문개정 2012. 1. 26.]\n",
      " \n",
      "제45조 삭제 <2017. 12. 19.>\n",
      " \n",
      "제45조의2 삭제 <2017. 12. 19.>\n",
      "        \n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "            [법령 위치]\n",
      "            법률명: \n",
      "            장: 제1장 총칙\n",
      "            조문: 제1조\n",
      "            \n",
      "            [조문 내용]\n",
      "            이 법은 장애인의 인간다운 삶과 권리보장을 위한 국가와 지방자치단체 등의 책임을 명백히 하고, 장애발\n",
      "생 예방과 장애인의 의료ㆍ교육ㆍ직업재활ㆍ생활환경개선 등에 관한 사업을 정하여 장애인복지대책을 종합적으로\n",
      "추진하며, 장애인의 자립생활ㆍ보호 및 수당지급 등에 관하여 필요한 사항을 정하여 장애인의 생활안정에 기여하는\n",
      "등 장애인의 복지와 사회활동 참여증진을 통하여 사회통합에 이바지함을 목적으로 한다.\n",
      "        \n",
      "\n",
      "[질문]\n",
      "장애인복지법 제59조의7에는 무엇이 명시되어 있나요?\n",
      "\n",
      "[답변 규칙]\n",
      "1. 위 법령 내용만을 기반으로 답변합니다.\n",
      "2. 답변의 근거가 되는 조항(예: 제00조 제0항)을 반드시 먼저 명시합니다.\n",
      "3. 조항의 내용을 직접 인용하면서 설명합니다.\n",
      "4. 법령에 명시되지 않은 내용은 추정하거나 해석하지 않고, \"해당 내용은 제시된 법령에 명시되어 있지 않습니다\"라고 답변합니다.\n",
      "\n",
      "[답변] \n",
      "장애인복지법 제59조의7은 다음과 같이 명시되어 있습니다:\n",
      "\"장애인학대 및 장애인 대상 성범죄 신고인에 대하여는 '특정범죄 신고자 등 보호법' 제7조부터 제13조까지의 규정을 준용한다.\" \n",
      "\n",
      "따라서 해당 조항에 따르면, 장애인 학대 및 장애인 대상 성범죄를 신고한 경우에는 '특정범죄 신고자 등 보호법' 제7조부터 제13조까지의 규정이 적용됩니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def find_article(question: str) -> dict:\n",
    "    \"\"\"질문에서 조항 번호를 추출하고 해당 필터를 반환\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # 조항 번호 패턴 매칭 (예: 제59조의9, 제2조 등)\n",
    "    pattern = r'제(\\d+)조의?(\\d+)?'\n",
    "    match = re.search(pattern, question)\n",
    "    \n",
    "    if match:\n",
    "        article_num = match.group(0)\n",
    "        return {\"article_number\": article_num}\n",
    "    return None\n",
    "\n",
    "# 3. retrieval_chain 수정\n",
    "retrieval_chain = RunnableParallel({\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"retrieved_docs\": RunnableLambda(\n",
    "        lambda x: format_docs(\n",
    "            retriever.get_relevant_documents(\n",
    "                x,\n",
    "                search_kwargs={\n",
    "                    \"filter\": find_article(x)\n",
    "                } if find_article(x) else {}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "})\n",
    "\n",
    "# 4. prompt 템플릿 수정\n",
    "template = \"\"\"장애인복지법 관련 질의응답을 진행하겠습니다.\n",
    "[법령 기본 정보]\n",
    "법률명: {law_title}\n",
    "문서 유형: {document_type}\n",
    "시행일자: {effective_date}\n",
    "법률 분야: {legal_area}\n",
    "\n",
    "[참고할 법령 내용]\n",
    "{context}\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "[답변 형식]\n",
    "1. 먼저 해당 조항의 존재 여부를 명시해 주세요.\n",
    "2. 조항이 존재하는 경우, 조항 번호와 전체 내용을 인용해 주세요.\n",
    "3. 조항이 없는 경우, \"해당 조항은 제시된 법령에 포함되어 있지 않습니다\"라고 답변해 주세요.\n",
    "\n",
    "[답변]\"\"\"\n",
    "\n",
    "# 5. 실행 시 에러 처리 추가\n",
    "def safe_invoke(question: str):\n",
    "    try:\n",
    "        return rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"검색 중 오류가 발생했습니다. 해당 조항이 법령에 포함되어 있지 않을 수 있습니다. 오류: {str(e)}\"\n",
    "\n",
    "# 테스트\n",
    "test_question = \"장애인복지법 제59조의7에는 무엇이 명시되어 있나요?\"\n",
    "answer = safe_invoke(test_question)\n",
    "print(f\"질문: {test_question}\")\n",
    "print(f\"답변: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "service-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
